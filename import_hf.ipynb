{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c93852b705246ae8894f8f5d0f97f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1753363caa48369c734890552af367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)d7450235/config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66afc736dfd9441092489dd914b88198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json             special_tokens_map.json tokenizer.model\n",
      "generation_config.json  tokenizer.json          tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "!pip install -q huggingface_hub\n",
    "import torch\n",
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "version = 15 # 15, 42, or 110\n",
    "output_folder = f\"llama2.c-stories{version}M\"\n",
    "\n",
    "snapshot_download(\n",
    "\t\trepo_id=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "\t\tallow_patterns=[\"*.json\", \"tokenizer.model\"],\n",
    "\t\tignore_patterns=[\"model.safetensors.index.json\", 'pytorch_model.bin.index.json'],\n",
    "\t\tlocal_dir=output_folder,\n",
    "\t\tlocal_dir_use_symlinks=False\n",
    ")\n",
    "!ls $output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.embed_tokens.weight',\n",
       " 'model.layers.0.self_attn.q_proj.weight',\n",
       " 'model.layers.0.self_attn.k_proj.weight',\n",
       " 'model.layers.0.self_attn.v_proj.weight',\n",
       " 'model.layers.0.self_attn.o_proj.weight']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.load(f\"stories{version}M.pt\")\n",
    "\n",
    "if data['config'].get('vocab_source', 'llama2') != 'llama2':\n",
    "\traise NotImplementedError(\"No custom vocab support yet\")\n",
    "\n",
    "renamed_data = {}\n",
    "for k, v in data[\"model\"].items():\n",
    "    k = k.replace(\"layers\", \"model.layers\")\n",
    "    k = k.replace(\".feed_forward.w1\", \".mlp.gate_proj\")\n",
    "    k = k.replace(\".feed_forward.w2\", \".mlp.down_proj\")\n",
    "    k = k.replace(\".feed_forward.w3\", \".mlp.up_proj\")\n",
    "    k = k.replace(\".attention.wq\", \".self_attn.q_proj\")\n",
    "    k = k.replace(\".attention.wk\", \".self_attn.k_proj\")\n",
    "    k = k.replace(\".attention.wv\", \".self_attn.v_proj\")\n",
    "    k = k.replace(\".attention.wo\", \".self_attn.o_proj\")\n",
    "    k = k.replace(\"norm.weight\", \"model.norm.weight\")\n",
    "    k = k.replace(\"output.weight\", \"lm_head.weight\")\n",
    "    k = k.replace(\"ffn_model.norm\", \"post_attention_layernorm\")\n",
    "    k = k.replace(\"attention_model.norm\", \"input_layernorm\")\n",
    "    k = k.replace(\"tok_embeddings.weight\", \"model.embed_tokens.weight\")\n",
    "    renamed_data[k] = v\n",
    "\n",
    "list(renamed_data.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_folder, exist_ok=True)\n",
    "torch.save(renamed_data, f\"{output_folder}/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dim': 288,\n",
       " 'n_layers': 6,\n",
       " 'n_heads': 6,\n",
       " 'n_kv_heads': 6,\n",
       " 'vocab_size': 32000,\n",
       " 'hidden_dim': 768,\n",
       " 'multiple_of': 32,\n",
       " 'norm_eps': 1e-05,\n",
       " 'max_seq_len': 256,\n",
       " 'dropout': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import ModelArgs\n",
    "from dataclasses import asdict\n",
    "\n",
    "args = ModelArgs(**data['model_args'])\n",
    "args = asdict(args)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tie_word_embeddings': True,\n",
       " 'hidden_size': 288,\n",
       " 'num_attention_heads': 6,\n",
       " 'num_hidden_layers': 6,\n",
       " 'max_position_embeddings': 256,\n",
       " 'vocab_size': 32000,\n",
       " 'intermediate_size': 768,\n",
       " 'num_key_value_heads': 6,\n",
       " 'rms_norm_eps': 1e-05}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args_map = {\n",
    "    'dim': 'hidden_size',\n",
    "    'n_heads': 'num_attention_heads',\n",
    "    'n_layers': 'num_hidden_layers',\n",
    "    'max_seq_len': 'max_position_embeddings',\n",
    "    'vocab_size': 'vocab_size',\n",
    "    'hidden_dim': 'intermediate_size',\n",
    "    'n_kv_heads': 'num_key_value_heads',\n",
    "    'norm_eps': 'rms_norm_eps',\n",
    "}\n",
    "\n",
    "renamed_args = { 'tie_word_embeddings': True }\n",
    "for k,v in args_map.items():\n",
    "    renamed_args[v] = args.get(k)\n",
    "\n",
    "renamed_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 288,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 768,\n",
       "  \"max_position_embeddings\": 256,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 6,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"num_key_value_heads\": 6,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"transformers_version\": \"4.32.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -q transformers\n",
    "from transformers import LlamaConfig\n",
    "\n",
    "config = LlamaConfig(**renamed_args)\n",
    "config.save_pretrained(output_folder)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tokenizer_config = {\n",
    "  \"bos_token\": {\n",
    "    \"__type\": \"AddedToken\",\n",
    "    \"content\": \"<s>\",\n",
    "    \"lstrip\": False,\n",
    "    \"normalized\": True,\n",
    "    \"rstrip\": False,\n",
    "    \"single_word\": False\n",
    "  },\n",
    "  \"clean_up_tokenization_spaces\": False,\n",
    "  \"eos_token\": {\n",
    "    \"__type\": \"AddedToken\",\n",
    "    \"content\": \"</s>\",\n",
    "    \"lstrip\": False,\n",
    "    \"normalized\": True,\n",
    "    \"rstrip\": False,\n",
    "    \"single_word\": False\n",
    "  },\n",
    "  \"model_max_length\": args['max_seq_len'],\n",
    "  \"pad_token\": None,\n",
    "  \"sp_model_kwargs\": {},\n",
    "  \"tokenizer_class\": \"LlamaTokenizer\",\n",
    "  \"unk_token\": {\n",
    "    \"__type\": \"AddedToken\",\n",
    "    \"content\": \"<unk>\",\n",
    "    \"lstrip\": False,\n",
    "    \"normalized\": True,\n",
    "    \"rstrip\": False,\n",
    "    \"single_word\": False\n",
    "  },\n",
    "  \"use_default_system_prompt\": True\n",
    "}\n",
    "\n",
    "with open(f\"{output_folder}/tokenizer_config.json\", 'w') as f:\n",
    "\t\tjson.dump(tokenizer_config, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config_path = f\"{output_folder}/generation_config.json\"\n",
    "\n",
    "# Assert `generation_config.json` in folder\n",
    "assert os.path.exists(gen_config_path)\n",
    "\n",
    "# Overwrite`max_length` to 256\n",
    "with open(gen_config_path, \"r\") as f:\n",
    "\t\tconfig = json.load(f)\n",
    "\t\tconfig[\"max_length\"] = 256\n",
    "\t\tconfig['temperature'] = 0.0001\n",
    "\t\tconfig['top_p'] = 1\n",
    "with open(gen_config_path, \"w\") as f:\n",
    "\t\tjson.dump(config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp tokenizer.model $output_folder/tokenizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Lily was a squir toy of a girl who liked toy. She was a girl. She was her name her favorite friend. She was a boy. She was three-ar. She had a bit. She was aunt, too. She was called Lily, butter. She was a little girl who was three-d-die was three.\n",
      "One day, and she was Lily. She was called her mommy. She was called her age.\n",
      "One day, and she was called her first. She was called Lily'dusted. She was a bitter. She was a little girl.\n",
      "One day. She was a bit-dies. She was a little Lily. She was three years old.\n",
      "One day, and she was a bit- a bit-Mine and she was a bit. She was three years old. She was a bit. She was a bit-Lola.\n",
      "One day, and she was a bitter. She was a bit, three-eyed upstie. She was a bitter. She was a bit-f, she was a little girl.\n",
      "One day, and she was a bit. She was a bit.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "output_folder = 'Xenova/llama2.c-stories15M'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_folder) \n",
    "model = AutoModelForCausalLM.from_pretrained(output_folder)\n",
    "\n",
    "inputs = tokenizer.encode(\"Lily was\", return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_length=256)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "# <s> Once upon a time, there was a big bear named Benny was a very big bear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
